### Predicting Heart Failre Survival Using K-Nearest Neighbors (KNN) ###


# Table of Contents
1. [Introduction](#Introduction)
2. [Data Collection & Loading](#Data-Collection-&-Loading)
3. [Data Validation](#Data-Validation)
4. [Feature Selection](#Feature-Selection)
5. [Data Preprocessing](#Data-Preprocessing)
6. [Implementing KNN Model](#Implementing-KNN-Model)
7. [Hyperparameter Optimization](#Hyperparameter-Optimization) 
   - 7.1 [k-number of neighbors](#k-number-of-neighbors) 
   - 7.2 [Estimating the Distance method](#Estimating-the-Distance-method) 
   - 7.3 [Weighting Method](#Weighting-Method)
8. [Model Evaluation](#Model-Evaluation)
9. [Comparison with the Original Paper](#Comparison-with-the-Original-Paper)
10. [Conclusion & Future Work](#Conclusion-&-Future-Work)
6. [References](#References)


## Introduction ##

The primary objective of this project is to develop a machine learning model capable of predicting the survival or death of heart failure patients based on a set of clinical features. We employed the K-Nearest Neighbors (KNN) algorithm for this task.

The dataset was sourced from the UCI Machine Learning Repository, which provides open-access data. After verifying the cleanliness of the data, we assessed its balance, preprocessed it, and applied normalization to mitigate potential bias. We then optimized the model parameters based on four key performance metrics: accuracy, precision, recall, and F1-score. The resulting model was evaluated and compared with the findings of the primary reference paper [2]. Our results demonstrate a significant improvement over those reported in the reference, likely attributable to our model's parameter optimization.

It should be noted that this project does not perform feature selection and instead adopts the same features as those selected in the main reference [2].

## Data Collection & Loading ##

We have used a public database from UCI Machine Learning Repository called "Heart Failure Clinical Records" [1]. This database includes data about 299 patiens who had heart failure. thirteen clinical features are provided for every record. 


```python
import pandas as pd
df = pd.read_csv("heart_failure_clinical_records_dataset.csv")
df.head(10)
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>age</th>
      <th>anaemia</th>
      <th>creatinine_phosphokinase</th>
      <th>diabetes</th>
      <th>ejection_fraction</th>
      <th>high_blood_pressure</th>
      <th>platelets</th>
      <th>serum_creatinine</th>
      <th>serum_sodium</th>
      <th>sex</th>
      <th>smoking</th>
      <th>time</th>
      <th>DEATH_EVENT</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>75.0</td>
      <td>0</td>
      <td>582</td>
      <td>0</td>
      <td>20</td>
      <td>1</td>
      <td>265000.00</td>
      <td>1.9</td>
      <td>130</td>
      <td>1</td>
      <td>0</td>
      <td>4</td>
      <td>1</td>
    </tr>
    <tr>
      <th>1</th>
      <td>55.0</td>
      <td>0</td>
      <td>7861</td>
      <td>0</td>
      <td>38</td>
      <td>0</td>
      <td>263358.03</td>
      <td>1.1</td>
      <td>136</td>
      <td>1</td>
      <td>0</td>
      <td>6</td>
      <td>1</td>
    </tr>
    <tr>
      <th>2</th>
      <td>65.0</td>
      <td>0</td>
      <td>146</td>
      <td>0</td>
      <td>20</td>
      <td>0</td>
      <td>162000.00</td>
      <td>1.3</td>
      <td>129</td>
      <td>1</td>
      <td>1</td>
      <td>7</td>
      <td>1</td>
    </tr>
    <tr>
      <th>3</th>
      <td>50.0</td>
      <td>1</td>
      <td>111</td>
      <td>0</td>
      <td>20</td>
      <td>0</td>
      <td>210000.00</td>
      <td>1.9</td>
      <td>137</td>
      <td>1</td>
      <td>0</td>
      <td>7</td>
      <td>1</td>
    </tr>
    <tr>
      <th>4</th>
      <td>65.0</td>
      <td>1</td>
      <td>160</td>
      <td>1</td>
      <td>20</td>
      <td>0</td>
      <td>327000.00</td>
      <td>2.7</td>
      <td>116</td>
      <td>0</td>
      <td>0</td>
      <td>8</td>
      <td>1</td>
    </tr>
    <tr>
      <th>5</th>
      <td>90.0</td>
      <td>1</td>
      <td>47</td>
      <td>0</td>
      <td>40</td>
      <td>1</td>
      <td>204000.00</td>
      <td>2.1</td>
      <td>132</td>
      <td>1</td>
      <td>1</td>
      <td>8</td>
      <td>1</td>
    </tr>
    <tr>
      <th>6</th>
      <td>75.0</td>
      <td>1</td>
      <td>246</td>
      <td>0</td>
      <td>15</td>
      <td>0</td>
      <td>127000.00</td>
      <td>1.2</td>
      <td>137</td>
      <td>1</td>
      <td>0</td>
      <td>10</td>
      <td>1</td>
    </tr>
    <tr>
      <th>7</th>
      <td>60.0</td>
      <td>1</td>
      <td>315</td>
      <td>1</td>
      <td>60</td>
      <td>0</td>
      <td>454000.00</td>
      <td>1.1</td>
      <td>131</td>
      <td>1</td>
      <td>1</td>
      <td>10</td>
      <td>1</td>
    </tr>
    <tr>
      <th>8</th>
      <td>65.0</td>
      <td>0</td>
      <td>157</td>
      <td>0</td>
      <td>65</td>
      <td>0</td>
      <td>263358.03</td>
      <td>1.5</td>
      <td>138</td>
      <td>0</td>
      <td>0</td>
      <td>10</td>
      <td>1</td>
    </tr>
    <tr>
      <th>9</th>
      <td>80.0</td>
      <td>1</td>
      <td>123</td>
      <td>0</td>
      <td>35</td>
      <td>1</td>
      <td>388000.00</td>
      <td>9.4</td>
      <td>133</td>
      <td>1</td>
      <td>1</td>
      <td>10</td>
      <td>1</td>
    </tr>
  </tbody>
</table>
</div>



As it is shown in the above table, the following fields are provided:


```python
df.columns.tolist()
```




    ['age',
     'anaemia',
     'creatinine_phosphokinase',
     'diabetes',
     'ejection_fraction',
     'high_blood_pressure',
     'platelets',
     'serum_creatinine',
     'serum_sodium',
     'sex',
     'smoking',
     'time',
     'DEATH_EVENT']



## Data Validation ##

to validate the data we can check throguh the following codes:


```python
df.isnull().sum()
```




    age                         0
    anaemia                     0
    creatinine_phosphokinase    0
    diabetes                    0
    ejection_fraction           0
    high_blood_pressure         0
    platelets                   0
    serum_creatinine            0
    serum_sodium                0
    sex                         0
    smoking                     0
    time                        0
    DEATH_EVENT                 0
    dtype: int64



So, no null cell has been detected.


```python
df.dtypes
```




    age                         float64
    anaemia                       int64
    creatinine_phosphokinase      int64
    diabetes                      int64
    ejection_fraction             int64
    high_blood_pressure           int64
    platelets                   float64
    serum_creatinine            float64
    serum_sodium                  int64
    sex                           int64
    smoking                       int64
    time                          int64
    DEATH_EVENT                   int64
    dtype: object



There is no "object" dtype, which is usually because of the multiple data type in a column. in the other words, if a column of type int64, has a wrong value of type text, the column dtype will be shown as object. So, our data-frame seems to be pure, from this point of view.


```python
for column in df.select_dtypes(include=['int64','float64']).columns:
    print(f"{column}: Min = {df[column].min()}, Max = {df[column].max()}")
```

    age: Min = 40.0, Max = 95.0
    anaemia: Min = 0, Max = 1
    creatinine_phosphokinase: Min = 23, Max = 7861
    diabetes: Min = 0, Max = 1
    ejection_fraction: Min = 14, Max = 80
    high_blood_pressure: Min = 0, Max = 1
    platelets: Min = 25100.0, Max = 850000.0
    serum_creatinine: Min = 0.5, Max = 9.4
    serum_sodium: Min = 113, Max = 148
    sex: Min = 0, Max = 1
    smoking: Min = 0, Max = 1
    time: Min = 4, Max = 285
    DEATH_EVENT: Min = 0, Max = 1
    

the data range of numeric cells is OK.


```python
df.describe()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>age</th>
      <th>anaemia</th>
      <th>creatinine_phosphokinase</th>
      <th>diabetes</th>
      <th>ejection_fraction</th>
      <th>high_blood_pressure</th>
      <th>platelets</th>
      <th>serum_creatinine</th>
      <th>serum_sodium</th>
      <th>sex</th>
      <th>smoking</th>
      <th>time</th>
      <th>DEATH_EVENT</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>count</th>
      <td>299.000000</td>
      <td>299.000000</td>
      <td>299.000000</td>
      <td>299.000000</td>
      <td>299.000000</td>
      <td>299.000000</td>
      <td>299.000000</td>
      <td>299.00000</td>
      <td>299.000000</td>
      <td>299.000000</td>
      <td>299.00000</td>
      <td>299.000000</td>
      <td>299.00000</td>
    </tr>
    <tr>
      <th>mean</th>
      <td>60.833893</td>
      <td>0.431438</td>
      <td>581.839465</td>
      <td>0.418060</td>
      <td>38.083612</td>
      <td>0.351171</td>
      <td>263358.029264</td>
      <td>1.39388</td>
      <td>136.625418</td>
      <td>0.648829</td>
      <td>0.32107</td>
      <td>130.260870</td>
      <td>0.32107</td>
    </tr>
    <tr>
      <th>std</th>
      <td>11.894809</td>
      <td>0.496107</td>
      <td>970.287881</td>
      <td>0.494067</td>
      <td>11.834841</td>
      <td>0.478136</td>
      <td>97804.236869</td>
      <td>1.03451</td>
      <td>4.412477</td>
      <td>0.478136</td>
      <td>0.46767</td>
      <td>77.614208</td>
      <td>0.46767</td>
    </tr>
    <tr>
      <th>min</th>
      <td>40.000000</td>
      <td>0.000000</td>
      <td>23.000000</td>
      <td>0.000000</td>
      <td>14.000000</td>
      <td>0.000000</td>
      <td>25100.000000</td>
      <td>0.50000</td>
      <td>113.000000</td>
      <td>0.000000</td>
      <td>0.00000</td>
      <td>4.000000</td>
      <td>0.00000</td>
    </tr>
    <tr>
      <th>25%</th>
      <td>51.000000</td>
      <td>0.000000</td>
      <td>116.500000</td>
      <td>0.000000</td>
      <td>30.000000</td>
      <td>0.000000</td>
      <td>212500.000000</td>
      <td>0.90000</td>
      <td>134.000000</td>
      <td>0.000000</td>
      <td>0.00000</td>
      <td>73.000000</td>
      <td>0.00000</td>
    </tr>
    <tr>
      <th>50%</th>
      <td>60.000000</td>
      <td>0.000000</td>
      <td>250.000000</td>
      <td>0.000000</td>
      <td>38.000000</td>
      <td>0.000000</td>
      <td>262000.000000</td>
      <td>1.10000</td>
      <td>137.000000</td>
      <td>1.000000</td>
      <td>0.00000</td>
      <td>115.000000</td>
      <td>0.00000</td>
    </tr>
    <tr>
      <th>75%</th>
      <td>70.000000</td>
      <td>1.000000</td>
      <td>582.000000</td>
      <td>1.000000</td>
      <td>45.000000</td>
      <td>1.000000</td>
      <td>303500.000000</td>
      <td>1.40000</td>
      <td>140.000000</td>
      <td>1.000000</td>
      <td>1.00000</td>
      <td>203.000000</td>
      <td>1.00000</td>
    </tr>
    <tr>
      <th>max</th>
      <td>95.000000</td>
      <td>1.000000</td>
      <td>7861.000000</td>
      <td>1.000000</td>
      <td>80.000000</td>
      <td>1.000000</td>
      <td>850000.000000</td>
      <td>9.40000</td>
      <td>148.000000</td>
      <td>1.000000</td>
      <td>1.00000</td>
      <td>285.000000</td>
      <td>1.00000</td>
    </tr>
  </tbody>
</table>
</div>



So, we can confirm the dataframe is properly prepared, and there is no need for extra-prepration or purification.

## Feature Selection ##

According to the main reference of this project [2], the main two features <span style="font-weight:bold">"serum ceratinine"</span> and <span style="font-weight:bold">"ejection fraction"</span> are sufficient to predict the survival of heart failure patients. At the moment, we neglect the statistical process that leads to the proof of this statement.   


```python
X = df[["serum_creatinine","ejection_fraction"]]
y = df[["DEATH_EVENT"]]
```

## Data Preprocessing ##

The data we want to work are as follows:


```python
print(f"{"Item"} \t {"serum_creatinine"} \t {"ejection_fraction"} \t {"DEATH_EVENT"}")
for index, row in pd.concat([X, y], axis=1).iterrows():
    print(f"{index} \t {row["serum_creatinine"]} \t\t\t\t {row["ejection_fraction"]} \t\t\t {row["DEATH_EVENT"]}")
```

    Item 	 serum_creatinine 	 ejection_fraction 	 DEATH_EVENT
    0 	 1.9 				 20.0 			 1.0
    1 	 1.1 				 38.0 			 1.0
    2 	 1.3 				 20.0 			 1.0
    3 	 1.9 				 20.0 			 1.0
    4 	 2.7 				 20.0 			 1.0
    5 	 2.1 				 40.0 			 1.0
    6 	 1.2 				 15.0 			 1.0
    7 	 1.1 				 60.0 			 1.0
    8 	 1.5 				 65.0 			 1.0
    9 	 9.4 				 35.0 			 1.0
    10 	 4.0 				 38.0 			 1.0
    11 	 0.9 				 25.0 			 1.0
    12 	 1.1 				 30.0 			 1.0
    13 	 1.1 				 38.0 			 1.0
    14 	 1.0 				 30.0 			 0.0
    15 	 1.3 				 50.0 			 1.0
    16 	 0.9 				 38.0 			 1.0
    17 	 0.8 				 14.0 			 1.0
    18 	 1.0 				 25.0 			 1.0
    19 	 1.9 				 55.0 			 1.0
    20 	 1.3 				 25.0 			 0.0
    21 	 1.6 				 30.0 			 1.0
    22 	 0.9 				 35.0 			 1.0
    23 	 0.8 				 60.0 			 0.0
    24 	 1.83 				 30.0 			 1.0
    25 	 1.9 				 38.0 			 1.0
    26 	 1.0 				 40.0 			 1.0
    27 	 1.3 				 45.0 			 1.0
    28 	 5.8 				 38.0 			 1.0
    29 	 1.2 				 30.0 			 1.0
    30 	 1.83 				 38.0 			 1.0
    31 	 3.0 				 45.0 			 1.0
    32 	 1.0 				 35.0 			 1.0
    33 	 1.2 				 30.0 			 0.0
    34 	 1.0 				 50.0 			 1.0
    35 	 3.5 				 35.0 			 1.0
    36 	 1.0 				 50.0 			 1.0
    37 	 1.0 				 50.0 			 1.0
    38 	 2.3 				 30.0 			 0.0
    39 	 3.0 				 38.0 			 1.0
    40 	 1.83 				 20.0 			 1.0
    41 	 1.2 				 30.0 			 1.0
    42 	 1.2 				 45.0 			 1.0
    43 	 1.0 				 50.0 			 0.0
    44 	 1.1 				 60.0 			 1.0
    45 	 1.9 				 38.0 			 1.0
    46 	 0.9 				 25.0 			 1.0
    47 	 0.6 				 38.0 			 1.0
    48 	 4.4 				 20.0 			 1.0
    49 	 1.0 				 30.0 			 1.0
    50 	 1.0 				 25.0 			 1.0
    51 	 1.4 				 20.0 			 1.0
    52 	 6.8 				 62.0 			 1.0
    53 	 1.0 				 50.0 			 1.0
    54 	 2.2 				 38.0 			 1.0
    55 	 2.0 				 30.0 			 1.0
    56 	 2.7 				 35.0 			 0.0
    57 	 0.6 				 40.0 			 0.0
    58 	 1.1 				 20.0 			 1.0
    59 	 1.3 				 20.0 			 1.0
    60 	 1.0 				 25.0 			 1.0
    61 	 2.3 				 40.0 			 1.0
    62 	 1.1 				 35.0 			 0.0
    63 	 1.0 				 35.0 			 1.0
    64 	 1.18 				 80.0 			 0.0
    65 	 2.9 				 20.0 			 1.0
    66 	 1.3 				 15.0 			 1.0
    67 	 1.0 				 25.0 			 1.0
    68 	 1.2 				 25.0 			 1.0
    69 	 1.83 				 25.0 			 1.0
    70 	 0.8 				 40.0 			 0.0
    71 	 0.9 				 35.0 			 0.0
    72 	 1.0 				 35.0 			 1.0
    73 	 1.3 				 50.0 			 0.0
    74 	 1.2 				 20.0 			 1.0
    75 	 0.7 				 20.0 			 1.0
    76 	 0.8 				 60.0 			 0.0
    77 	 1.2 				 40.0 			 0.0
    78 	 0.6 				 38.0 			 0.0
    79 	 0.9 				 45.0 			 0.0
    80 	 1.7 				 40.0 			 0.0
    81 	 1.18 				 50.0 			 0.0
    82 	 2.5 				 25.0 			 1.0
    83 	 1.8 				 50.0 			 0.0
    84 	 1.0 				 25.0 			 1.0
    85 	 0.7 				 50.0 			 0.0
    86 	 1.1 				 35.0 			 0.0
    87 	 0.8 				 60.0 			 0.0
    88 	 0.7 				 40.0 			 0.0
    89 	 1.1 				 25.0 			 0.0
    90 	 0.8 				 45.0 			 0.0
    91 	 1.0 				 45.0 			 0.0
    92 	 1.18 				 60.0 			 0.0
    93 	 1.7 				 25.0 			 1.0
    94 	 0.7 				 38.0 			 0.0
    95 	 1.0 				 60.0 			 0.0
    96 	 1.3 				 25.0 			 0.0
    97 	 1.1 				 60.0 			 0.0
    98 	 1.2 				 25.0 			 0.0
    99 	 1.1 				 40.0 			 0.0
    100 	 1.1 				 25.0 			 0.0
    101 	 1.18 				 45.0 			 0.0
    102 	 1.1 				 25.0 			 0.0
    103 	 1.0 				 30.0 			 0.0
    104 	 2.3 				 50.0 			 0.0
    105 	 1.7 				 30.0 			 1.0
    106 	 1.3 				 45.0 			 0.0
    107 	 0.9 				 35.0 			 0.0
    108 	 1.1 				 38.0 			 0.0
    109 	 1.3 				 35.0 			 0.0
    110 	 1.2 				 60.0 			 1.0
    111 	 1.2 				 35.0 			 0.0
    112 	 1.6 				 25.0 			 0.0
    113 	 1.3 				 60.0 			 1.0
    114 	 1.2 				 40.0 			 0.0
    115 	 1.0 				 40.0 			 0.0
    116 	 0.7 				 60.0 			 0.0
    117 	 3.2 				 60.0 			 0.0
    118 	 0.9 				 60.0 			 0.0
    119 	 1.83 				 38.0 			 1.0
    120 	 1.5 				 60.0 			 0.0
    121 	 1.0 				 38.0 			 0.0
    122 	 0.75 				 38.0 			 0.0
    123 	 0.9 				 30.0 			 0.0
    124 	 3.7 				 40.0 			 1.0
    125 	 1.3 				 50.0 			 0.0
    126 	 2.1 				 17.0 			 1.0
    127 	 0.8 				 60.0 			 0.0
    128 	 0.7 				 30.0 			 0.0
    129 	 3.4 				 35.0 			 0.0
    130 	 0.7 				 60.0 			 0.0
    131 	 6.1 				 45.0 			 0.0
    132 	 1.18 				 40.0 			 0.0
    133 	 1.3 				 60.0 			 0.0
    134 	 1.18 				 35.0 			 0.0
    135 	 1.18 				 40.0 			 0.0
    136 	 0.9 				 60.0 			 0.0
    137 	 2.1 				 25.0 			 0.0
    138 	 1.0 				 35.0 			 0.0
    139 	 0.8 				 30.0 			 0.0
    140 	 1.1 				 38.0 			 1.0
    141 	 0.9 				 35.0 			 0.0
    142 	 0.9 				 30.0 			 0.0
    143 	 0.9 				 40.0 			 0.0
    144 	 1.7 				 25.0 			 1.0
    145 	 0.7 				 30.0 			 0.0
    146 	 0.7 				 30.0 			 0.0
    147 	 1.0 				 60.0 			 0.0
    148 	 1.83 				 30.0 			 1.0
    149 	 0.9 				 35.0 			 0.0
    150 	 2.5 				 45.0 			 1.0
    151 	 0.9 				 60.0 			 0.0
    152 	 0.9 				 45.0 			 0.0
    153 	 1.18 				 35.0 			 0.0
    154 	 0.8 				 35.0 			 0.0
    155 	 1.7 				 25.0 			 0.0
    156 	 1.4 				 35.0 			 0.0
    157 	 1.0 				 25.0 			 0.0
    158 	 1.3 				 50.0 			 0.0
    159 	 1.1 				 45.0 			 0.0
    160 	 1.2 				 40.0 			 0.0
    161 	 0.8 				 35.0 			 0.0
    162 	 0.9 				 40.0 			 0.0
    163 	 0.9 				 35.0 			 1.0
    164 	 1.1 				 30.0 			 1.0
    165 	 1.3 				 38.0 			 1.0
    166 	 0.7 				 60.0 			 0.0
    167 	 2.4 				 20.0 			 1.0
    168 	 1.0 				 40.0 			 0.0
    169 	 0.8 				 35.0 			 0.0
    170 	 1.5 				 35.0 			 0.0
    171 	 0.9 				 40.0 			 0.0
    172 	 1.1 				 60.0 			 0.0
    173 	 0.8 				 20.0 			 0.0
    174 	 0.9 				 35.0 			 0.0
    175 	 1.0 				 60.0 			 0.0
    176 	 1.0 				 40.0 			 0.0
    177 	 1.0 				 50.0 			 0.0
    178 	 1.2 				 60.0 			 0.0
    179 	 0.7 				 40.0 			 0.0
    180 	 0.9 				 30.0 			 0.0
    181 	 1.0 				 25.0 			 1.0
    182 	 1.2 				 25.0 			 1.0
    183 	 2.5 				 38.0 			 1.0
    184 	 1.2 				 25.0 			 1.0
    185 	 1.5 				 30.0 			 1.0
    186 	 0.6 				 50.0 			 1.0
    187 	 2.1 				 25.0 			 1.0
    188 	 1.0 				 40.0 			 0.0
    189 	 0.9 				 45.0 			 0.0
    190 	 2.1 				 35.0 			 0.0
    191 	 1.5 				 60.0 			 0.0
    192 	 0.7 				 40.0 			 0.0
    193 	 1.18 				 30.0 			 0.0
    194 	 1.6 				 20.0 			 1.0
    195 	 1.8 				 45.0 			 1.0
    196 	 1.18 				 38.0 			 0.0
    197 	 0.8 				 30.0 			 0.0
    198 	 1.0 				 20.0 			 0.0
    199 	 1.8 				 35.0 			 0.0
    200 	 0.7 				 45.0 			 0.0
    201 	 1.0 				 60.0 			 0.0
    202 	 0.9 				 60.0 			 0.0
    203 	 3.5 				 25.0 			 0.0
    204 	 0.7 				 40.0 			 0.0
    205 	 1.0 				 45.0 			 0.0
    206 	 0.8 				 40.0 			 0.0
    207 	 0.9 				 38.0 			 0.0
    208 	 1.0 				 40.0 			 0.0
    209 	 0.8 				 35.0 			 0.0
    210 	 1.0 				 17.0 			 0.0
    211 	 0.8 				 62.0 			 0.0
    212 	 1.4 				 50.0 			 0.0
    213 	 1.6 				 30.0 			 1.0
    214 	 0.8 				 35.0 			 0.0
    215 	 1.3 				 35.0 			 0.0
    216 	 0.9 				 50.0 			 0.0
    217 	 9.0 				 70.0 			 1.0
    218 	 1.1 				 35.0 			 0.0
    219 	 0.7 				 35.0 			 0.0
    220 	 1.83 				 20.0 			 1.0
    221 	 1.1 				 50.0 			 0.0
    222 	 1.1 				 35.0 			 0.0
    223 	 0.8 				 25.0 			 0.0
    224 	 1.0 				 25.0 			 0.0
    225 	 1.4 				 60.0 			 0.0
    226 	 1.3 				 25.0 			 0.0
    227 	 1.0 				 35.0 			 0.0
    228 	 5.0 				 25.0 			 0.0
    229 	 1.2 				 25.0 			 0.0
    230 	 1.7 				 30.0 			 1.0
    231 	 1.1 				 35.0 			 0.0
    232 	 0.9 				 35.0 			 0.0
    233 	 1.4 				 38.0 			 0.0
    234 	 1.1 				 45.0 			 0.0
    235 	 1.1 				 50.0 			 0.0
    236 	 1.1 				 50.0 			 0.0
    237 	 1.2 				 30.0 			 0.0
    238 	 1.0 				 40.0 			 0.0
    239 	 1.18 				 45.0 			 0.0
    240 	 1.3 				 35.0 			 0.0
    241 	 1.3 				 30.0 			 0.0
    242 	 1.1 				 35.0 			 0.0
    243 	 0.9 				 40.0 			 0.0
    244 	 1.8 				 38.0 			 0.0
    245 	 1.4 				 38.0 			 0.0
    246 	 1.1 				 25.0 			 1.0
    247 	 2.4 				 25.0 			 0.0
    248 	 1.0 				 35.0 			 0.0
    249 	 1.2 				 40.0 			 0.0
    250 	 0.5 				 30.0 			 0.0
    251 	 0.8 				 35.0 			 0.0
    252 	 1.0 				 45.0 			 0.0
    253 	 1.2 				 35.0 			 0.0
    254 	 1.0 				 60.0 			 0.0
    255 	 1.0 				 30.0 			 0.0
    256 	 1.7 				 38.0 			 0.0
    257 	 1.0 				 38.0 			 0.0
    258 	 0.8 				 25.0 			 0.0
    259 	 0.7 				 50.0 			 0.0
    260 	 1.0 				 40.0 			 0.0
    261 	 0.7 				 40.0 			 0.0
    262 	 1.4 				 25.0 			 1.0
    263 	 1.0 				 60.0 			 0.0
    264 	 1.2 				 38.0 			 0.0
    265 	 0.9 				 35.0 			 0.0
    266 	 1.83 				 20.0 			 1.0
    267 	 1.7 				 38.0 			 0.0
    268 	 0.9 				 38.0 			 0.0
    269 	 1.0 				 35.0 			 0.0
    270 	 1.6 				 30.0 			 0.0
    271 	 0.9 				 40.0 			 0.0
    272 	 1.2 				 38.0 			 0.0
    273 	 0.7 				 40.0 			 0.0
    274 	 1.0 				 30.0 			 0.0
    275 	 0.8 				 38.0 			 0.0
    276 	 1.1 				 35.0 			 0.0
    277 	 1.1 				 38.0 			 0.0
    278 	 0.7 				 30.0 			 0.0
    279 	 1.3 				 38.0 			 0.0
    280 	 1.0 				 40.0 			 0.0
    281 	 2.7 				 40.0 			 0.0
    282 	 3.8 				 30.0 			 0.0
    283 	 1.1 				 38.0 			 0.0
    284 	 0.8 				 40.0 			 0.0
    285 	 1.2 				 40.0 			 0.0
    286 	 1.7 				 35.0 			 0.0
    287 	 1.0 				 55.0 			 0.0
    288 	 1.1 				 35.0 			 0.0
    289 	 0.9 				 38.0 			 0.0
    290 	 0.8 				 55.0 			 0.0
    291 	 1.4 				 35.0 			 0.0
    292 	 1.0 				 38.0 			 0.0
    293 	 0.9 				 35.0 			 0.0
    294 	 1.1 				 38.0 			 0.0
    295 	 1.2 				 38.0 			 0.0
    296 	 0.8 				 60.0 			 0.0
    297 	 1.4 				 38.0 			 0.0
    298 	 1.6 				 45.0 			 0.0
    


```python
print(f"Min and Max values for serum_creatinine = {df["serum_creatinine"].min()} and {df["serum_creatinine"].max()}")
print(f"Min and Max values for ejection_fraction = {df["ejection_fraction"].min()} and {df["ejection_fraction"].max()}")
```

    Min and Max values for serum_creatinine = 0.5 and 9.4
    Min and Max values for ejection_fraction = 14 and 80
    

From the above provided data, it is clear that the numerical range of features are in different orders of magnitutes. So when estimating the distance between points, in KNN method, this may lead to domination of serum_creatinine feature, which its values are much more smaller. To fix this issue we will normalize the data. Initially we will use Min-Max Scaling method for normalization.


```python
from sklearn.preprocessing import MinMaxScaler
scaler = MinMaxScaler()
X_scaled = scaler.fit_transform(X)
X_scaled = pd.DataFrame(X_scaled, columns=['serum_creatinine', 'ejection_fraction'])
print(f"{"Item"} \t {"serum_creatinine"} \t\t\t\t {"ejection_fraction"} \t\t\t {"DEATH_EVENT"}")
for index, row in pd.concat([X_scaled, y], axis=1).iterrows():
    print(f"{index} \t {row["serum_creatinine"]} \t\t\t\t {row["ejection_fraction"]} \t\t\t {row["DEATH_EVENT"]}")

```

    Item 	 serum_creatinine 				 ejection_fraction 			 DEATH_EVENT
    0 	 0.15730337078651685 				 0.09090909090909091 			 1.0
    1 	 0.06741573033707866 				 0.36363636363636365 			 1.0
    2 	 0.0898876404494382 				 0.09090909090909091 			 1.0
    3 	 0.15730337078651685 				 0.09090909090909091 			 1.0
    4 	 0.24719101123595508 				 0.09090909090909091 			 1.0
    5 	 0.1797752808988764 				 0.3939393939393939 			 1.0
    6 	 0.07865168539325842 				 0.015151515151515166 			 1.0
    7 	 0.06741573033707866 				 0.696969696969697 			 1.0
    8 	 0.11235955056179775 				 0.7727272727272727 			 1.0
    9 	 0.9999999999999999 				 0.3181818181818181 			 1.0
    10 	 0.3932584269662921 				 0.36363636363636365 			 1.0
    11 	 0.0449438202247191 				 0.16666666666666666 			 1.0
    12 	 0.06741573033707866 				 0.24242424242424246 			 1.0
    13 	 0.06741573033707866 				 0.36363636363636365 			 1.0
    14 	 0.056179775280898875 				 0.24242424242424246 			 0.0
    15 	 0.0898876404494382 				 0.5454545454545454 			 1.0
    16 	 0.0449438202247191 				 0.36363636363636365 			 1.0
    17 	 0.033707865168539325 				 0.0 			 1.0
    18 	 0.056179775280898875 				 0.16666666666666666 			 1.0
    19 	 0.15730337078651685 				 0.6212121212121212 			 1.0
    20 	 0.0898876404494382 				 0.16666666666666666 			 0.0
    21 	 0.12359550561797752 				 0.24242424242424246 			 1.0
    22 	 0.0449438202247191 				 0.3181818181818181 			 1.0
    23 	 0.033707865168539325 				 0.696969696969697 			 0.0
    24 	 0.149438202247191 				 0.24242424242424246 			 1.0
    25 	 0.15730337078651685 				 0.36363636363636365 			 1.0
    26 	 0.056179775280898875 				 0.3939393939393939 			 1.0
    27 	 0.0898876404494382 				 0.4696969696969697 			 1.0
    28 	 0.595505617977528 				 0.36363636363636365 			 1.0
    29 	 0.07865168539325842 				 0.24242424242424246 			 1.0
    30 	 0.149438202247191 				 0.36363636363636365 			 1.0
    31 	 0.2808988764044944 				 0.4696969696969697 			 1.0
    32 	 0.056179775280898875 				 0.3181818181818181 			 1.0
    33 	 0.07865168539325842 				 0.24242424242424246 			 0.0
    34 	 0.056179775280898875 				 0.5454545454545454 			 1.0
    35 	 0.3370786516853932 				 0.3181818181818181 			 1.0
    36 	 0.056179775280898875 				 0.5454545454545454 			 1.0
    37 	 0.056179775280898875 				 0.5454545454545454 			 1.0
    38 	 0.20224719101123592 				 0.24242424242424246 			 0.0
    39 	 0.2808988764044944 				 0.36363636363636365 			 1.0
    40 	 0.149438202247191 				 0.09090909090909091 			 1.0
    41 	 0.07865168539325842 				 0.24242424242424246 			 1.0
    42 	 0.07865168539325842 				 0.4696969696969697 			 1.0
    43 	 0.056179775280898875 				 0.5454545454545454 			 0.0
    44 	 0.06741573033707866 				 0.696969696969697 			 1.0
    45 	 0.15730337078651685 				 0.36363636363636365 			 1.0
    46 	 0.0449438202247191 				 0.16666666666666666 			 1.0
    47 	 0.011235955056179775 				 0.36363636363636365 			 1.0
    48 	 0.4382022471910113 				 0.09090909090909091 			 1.0
    49 	 0.056179775280898875 				 0.24242424242424246 			 1.0
    50 	 0.056179775280898875 				 0.16666666666666666 			 1.0
    51 	 0.10112359550561797 				 0.09090909090909091 			 1.0
    52 	 0.7078651685393258 				 0.7272727272727273 			 1.0
    53 	 0.056179775280898875 				 0.5454545454545454 			 1.0
    54 	 0.1910112359550562 				 0.36363636363636365 			 1.0
    55 	 0.16853932584269662 				 0.24242424242424246 			 1.0
    56 	 0.24719101123595508 				 0.3181818181818181 			 0.0
    57 	 0.011235955056179775 				 0.3939393939393939 			 0.0
    58 	 0.06741573033707866 				 0.09090909090909091 			 1.0
    59 	 0.0898876404494382 				 0.09090909090909091 			 1.0
    60 	 0.056179775280898875 				 0.16666666666666666 			 1.0
    61 	 0.20224719101123592 				 0.3939393939393939 			 1.0
    62 	 0.06741573033707866 				 0.3181818181818181 			 0.0
    63 	 0.056179775280898875 				 0.3181818181818181 			 1.0
    64 	 0.07640449438202246 				 1.0 			 0.0
    65 	 0.2696629213483146 				 0.09090909090909091 			 1.0
    66 	 0.0898876404494382 				 0.015151515151515166 			 1.0
    67 	 0.056179775280898875 				 0.16666666666666666 			 1.0
    68 	 0.07865168539325842 				 0.16666666666666666 			 1.0
    69 	 0.149438202247191 				 0.16666666666666666 			 1.0
    70 	 0.033707865168539325 				 0.3939393939393939 			 0.0
    71 	 0.0449438202247191 				 0.3181818181818181 			 0.0
    72 	 0.056179775280898875 				 0.3181818181818181 			 1.0
    73 	 0.0898876404494382 				 0.5454545454545454 			 0.0
    74 	 0.07865168539325842 				 0.09090909090909091 			 1.0
    75 	 0.02247191011235955 				 0.09090909090909091 			 1.0
    76 	 0.033707865168539325 				 0.696969696969697 			 0.0
    77 	 0.07865168539325842 				 0.3939393939393939 			 0.0
    78 	 0.011235955056179775 				 0.36363636363636365 			 0.0
    79 	 0.0449438202247191 				 0.4696969696969697 			 0.0
    80 	 0.1348314606741573 				 0.3939393939393939 			 0.0
    81 	 0.07640449438202246 				 0.5454545454545454 			 0.0
    82 	 0.22471910112359553 				 0.16666666666666666 			 1.0
    83 	 0.14606741573033707 				 0.5454545454545454 			 0.0
    84 	 0.056179775280898875 				 0.16666666666666666 			 1.0
    85 	 0.02247191011235955 				 0.5454545454545454 			 0.0
    86 	 0.06741573033707866 				 0.3181818181818181 			 0.0
    87 	 0.033707865168539325 				 0.696969696969697 			 0.0
    88 	 0.02247191011235955 				 0.3939393939393939 			 0.0
    89 	 0.06741573033707866 				 0.16666666666666666 			 0.0
    90 	 0.033707865168539325 				 0.4696969696969697 			 0.0
    91 	 0.056179775280898875 				 0.4696969696969697 			 0.0
    92 	 0.07640449438202246 				 0.696969696969697 			 0.0
    93 	 0.1348314606741573 				 0.16666666666666666 			 1.0
    94 	 0.02247191011235955 				 0.36363636363636365 			 0.0
    95 	 0.056179775280898875 				 0.696969696969697 			 0.0
    96 	 0.0898876404494382 				 0.16666666666666666 			 0.0
    97 	 0.06741573033707866 				 0.696969696969697 			 0.0
    98 	 0.07865168539325842 				 0.16666666666666666 			 0.0
    99 	 0.06741573033707866 				 0.3939393939393939 			 0.0
    100 	 0.06741573033707866 				 0.16666666666666666 			 0.0
    101 	 0.07640449438202246 				 0.4696969696969697 			 0.0
    102 	 0.06741573033707866 				 0.16666666666666666 			 0.0
    103 	 0.056179775280898875 				 0.24242424242424246 			 0.0
    104 	 0.20224719101123592 				 0.5454545454545454 			 0.0
    105 	 0.1348314606741573 				 0.24242424242424246 			 1.0
    106 	 0.0898876404494382 				 0.4696969696969697 			 0.0
    107 	 0.0449438202247191 				 0.3181818181818181 			 0.0
    108 	 0.06741573033707866 				 0.36363636363636365 			 0.0
    109 	 0.0898876404494382 				 0.3181818181818181 			 0.0
    110 	 0.07865168539325842 				 0.696969696969697 			 1.0
    111 	 0.07865168539325842 				 0.3181818181818181 			 0.0
    112 	 0.12359550561797752 				 0.16666666666666666 			 0.0
    113 	 0.0898876404494382 				 0.696969696969697 			 1.0
    114 	 0.07865168539325842 				 0.3939393939393939 			 0.0
    115 	 0.056179775280898875 				 0.3939393939393939 			 0.0
    116 	 0.02247191011235955 				 0.696969696969697 			 0.0
    117 	 0.3033707865168539 				 0.696969696969697 			 0.0
    118 	 0.0449438202247191 				 0.696969696969697 			 0.0
    119 	 0.149438202247191 				 0.36363636363636365 			 1.0
    120 	 0.11235955056179775 				 0.696969696969697 			 0.0
    121 	 0.056179775280898875 				 0.36363636363636365 			 0.0
    122 	 0.028089887640449437 				 0.36363636363636365 			 0.0
    123 	 0.0449438202247191 				 0.24242424242424246 			 0.0
    124 	 0.3595505617977528 				 0.3939393939393939 			 1.0
    125 	 0.0898876404494382 				 0.5454545454545454 			 0.0
    126 	 0.1797752808988764 				 0.04545454545454544 			 1.0
    127 	 0.033707865168539325 				 0.696969696969697 			 0.0
    128 	 0.02247191011235955 				 0.24242424242424246 			 0.0
    129 	 0.3258426966292135 				 0.3181818181818181 			 0.0
    130 	 0.02247191011235955 				 0.696969696969697 			 0.0
    131 	 0.6292134831460673 				 0.4696969696969697 			 0.0
    132 	 0.07640449438202246 				 0.3939393939393939 			 0.0
    133 	 0.0898876404494382 				 0.696969696969697 			 0.0
    134 	 0.07640449438202246 				 0.3181818181818181 			 0.0
    135 	 0.07640449438202246 				 0.3939393939393939 			 0.0
    136 	 0.0449438202247191 				 0.696969696969697 			 0.0
    137 	 0.1797752808988764 				 0.16666666666666666 			 0.0
    138 	 0.056179775280898875 				 0.3181818181818181 			 0.0
    139 	 0.033707865168539325 				 0.24242424242424246 			 0.0
    140 	 0.06741573033707866 				 0.36363636363636365 			 1.0
    141 	 0.0449438202247191 				 0.3181818181818181 			 0.0
    142 	 0.0449438202247191 				 0.24242424242424246 			 0.0
    143 	 0.0449438202247191 				 0.3939393939393939 			 0.0
    144 	 0.1348314606741573 				 0.16666666666666666 			 1.0
    145 	 0.02247191011235955 				 0.24242424242424246 			 0.0
    146 	 0.02247191011235955 				 0.24242424242424246 			 0.0
    147 	 0.056179775280898875 				 0.696969696969697 			 0.0
    148 	 0.149438202247191 				 0.24242424242424246 			 1.0
    149 	 0.0449438202247191 				 0.3181818181818181 			 0.0
    150 	 0.22471910112359553 				 0.4696969696969697 			 1.0
    151 	 0.0449438202247191 				 0.696969696969697 			 0.0
    152 	 0.0449438202247191 				 0.4696969696969697 			 0.0
    153 	 0.07640449438202246 				 0.3181818181818181 			 0.0
    154 	 0.033707865168539325 				 0.3181818181818181 			 0.0
    155 	 0.1348314606741573 				 0.16666666666666666 			 0.0
    156 	 0.10112359550561797 				 0.3181818181818181 			 0.0
    157 	 0.056179775280898875 				 0.16666666666666666 			 0.0
    158 	 0.0898876404494382 				 0.5454545454545454 			 0.0
    159 	 0.06741573033707866 				 0.4696969696969697 			 0.0
    160 	 0.07865168539325842 				 0.3939393939393939 			 0.0
    161 	 0.033707865168539325 				 0.3181818181818181 			 0.0
    162 	 0.0449438202247191 				 0.3939393939393939 			 0.0
    163 	 0.0449438202247191 				 0.3181818181818181 			 1.0
    164 	 0.06741573033707866 				 0.24242424242424246 			 1.0
    165 	 0.0898876404494382 				 0.36363636363636365 			 1.0
    166 	 0.02247191011235955 				 0.696969696969697 			 0.0
    167 	 0.21348314606741572 				 0.09090909090909091 			 1.0
    168 	 0.056179775280898875 				 0.3939393939393939 			 0.0
    169 	 0.033707865168539325 				 0.3181818181818181 			 0.0
    170 	 0.11235955056179775 				 0.3181818181818181 			 0.0
    171 	 0.0449438202247191 				 0.3939393939393939 			 0.0
    172 	 0.06741573033707866 				 0.696969696969697 			 0.0
    173 	 0.033707865168539325 				 0.09090909090909091 			 0.0
    174 	 0.0449438202247191 				 0.3181818181818181 			 0.0
    175 	 0.056179775280898875 				 0.696969696969697 			 0.0
    176 	 0.056179775280898875 				 0.3939393939393939 			 0.0
    177 	 0.056179775280898875 				 0.5454545454545454 			 0.0
    178 	 0.07865168539325842 				 0.696969696969697 			 0.0
    179 	 0.02247191011235955 				 0.3939393939393939 			 0.0
    180 	 0.0449438202247191 				 0.24242424242424246 			 0.0
    181 	 0.056179775280898875 				 0.16666666666666666 			 1.0
    182 	 0.07865168539325842 				 0.16666666666666666 			 1.0
    183 	 0.22471910112359553 				 0.36363636363636365 			 1.0
    184 	 0.07865168539325842 				 0.16666666666666666 			 1.0
    185 	 0.11235955056179775 				 0.24242424242424246 			 1.0
    186 	 0.011235955056179775 				 0.5454545454545454 			 1.0
    187 	 0.1797752808988764 				 0.16666666666666666 			 1.0
    188 	 0.056179775280898875 				 0.3939393939393939 			 0.0
    189 	 0.0449438202247191 				 0.4696969696969697 			 0.0
    190 	 0.1797752808988764 				 0.3181818181818181 			 0.0
    191 	 0.11235955056179775 				 0.696969696969697 			 0.0
    192 	 0.02247191011235955 				 0.3939393939393939 			 0.0
    193 	 0.07640449438202246 				 0.24242424242424246 			 0.0
    194 	 0.12359550561797752 				 0.09090909090909091 			 1.0
    195 	 0.14606741573033707 				 0.4696969696969697 			 1.0
    196 	 0.07640449438202246 				 0.36363636363636365 			 0.0
    197 	 0.033707865168539325 				 0.24242424242424246 			 0.0
    198 	 0.056179775280898875 				 0.09090909090909091 			 0.0
    199 	 0.14606741573033707 				 0.3181818181818181 			 0.0
    200 	 0.02247191011235955 				 0.4696969696969697 			 0.0
    201 	 0.056179775280898875 				 0.696969696969697 			 0.0
    202 	 0.0449438202247191 				 0.696969696969697 			 0.0
    203 	 0.3370786516853932 				 0.16666666666666666 			 0.0
    204 	 0.02247191011235955 				 0.3939393939393939 			 0.0
    205 	 0.056179775280898875 				 0.4696969696969697 			 0.0
    206 	 0.033707865168539325 				 0.3939393939393939 			 0.0
    207 	 0.0449438202247191 				 0.36363636363636365 			 0.0
    208 	 0.056179775280898875 				 0.3939393939393939 			 0.0
    209 	 0.033707865168539325 				 0.3181818181818181 			 0.0
    210 	 0.056179775280898875 				 0.04545454545454544 			 0.0
    211 	 0.033707865168539325 				 0.7272727272727273 			 0.0
    212 	 0.10112359550561797 				 0.5454545454545454 			 0.0
    213 	 0.12359550561797752 				 0.24242424242424246 			 1.0
    214 	 0.033707865168539325 				 0.3181818181818181 			 0.0
    215 	 0.0898876404494382 				 0.3181818181818181 			 0.0
    216 	 0.0449438202247191 				 0.5454545454545454 			 0.0
    217 	 0.9550561797752809 				 0.8484848484848484 			 1.0
    218 	 0.06741573033707866 				 0.3181818181818181 			 0.0
    219 	 0.02247191011235955 				 0.3181818181818181 			 0.0
    220 	 0.149438202247191 				 0.09090909090909091 			 1.0
    221 	 0.06741573033707866 				 0.5454545454545454 			 0.0
    222 	 0.06741573033707866 				 0.3181818181818181 			 0.0
    223 	 0.033707865168539325 				 0.16666666666666666 			 0.0
    224 	 0.056179775280898875 				 0.16666666666666666 			 0.0
    225 	 0.10112359550561797 				 0.696969696969697 			 0.0
    226 	 0.0898876404494382 				 0.16666666666666666 			 0.0
    227 	 0.056179775280898875 				 0.3181818181818181 			 0.0
    228 	 0.5056179775280899 				 0.16666666666666666 			 0.0
    229 	 0.07865168539325842 				 0.16666666666666666 			 0.0
    230 	 0.1348314606741573 				 0.24242424242424246 			 1.0
    231 	 0.06741573033707866 				 0.3181818181818181 			 0.0
    232 	 0.0449438202247191 				 0.3181818181818181 			 0.0
    233 	 0.10112359550561797 				 0.36363636363636365 			 0.0
    234 	 0.06741573033707866 				 0.4696969696969697 			 0.0
    235 	 0.06741573033707866 				 0.5454545454545454 			 0.0
    236 	 0.06741573033707866 				 0.5454545454545454 			 0.0
    237 	 0.07865168539325842 				 0.24242424242424246 			 0.0
    238 	 0.056179775280898875 				 0.3939393939393939 			 0.0
    239 	 0.07640449438202246 				 0.4696969696969697 			 0.0
    240 	 0.0898876404494382 				 0.3181818181818181 			 0.0
    241 	 0.0898876404494382 				 0.24242424242424246 			 0.0
    242 	 0.06741573033707866 				 0.3181818181818181 			 0.0
    243 	 0.0449438202247191 				 0.3939393939393939 			 0.0
    244 	 0.14606741573033707 				 0.36363636363636365 			 0.0
    245 	 0.10112359550561797 				 0.36363636363636365 			 0.0
    246 	 0.06741573033707866 				 0.16666666666666666 			 1.0
    247 	 0.21348314606741572 				 0.16666666666666666 			 0.0
    248 	 0.056179775280898875 				 0.3181818181818181 			 0.0
    249 	 0.07865168539325842 				 0.3939393939393939 			 0.0
    250 	 0.0 				 0.24242424242424246 			 0.0
    251 	 0.033707865168539325 				 0.3181818181818181 			 0.0
    252 	 0.056179775280898875 				 0.4696969696969697 			 0.0
    253 	 0.07865168539325842 				 0.3181818181818181 			 0.0
    254 	 0.056179775280898875 				 0.696969696969697 			 0.0
    255 	 0.056179775280898875 				 0.24242424242424246 			 0.0
    256 	 0.1348314606741573 				 0.36363636363636365 			 0.0
    257 	 0.056179775280898875 				 0.36363636363636365 			 0.0
    258 	 0.033707865168539325 				 0.16666666666666666 			 0.0
    259 	 0.02247191011235955 				 0.5454545454545454 			 0.0
    260 	 0.056179775280898875 				 0.3939393939393939 			 0.0
    261 	 0.02247191011235955 				 0.3939393939393939 			 0.0
    262 	 0.10112359550561797 				 0.16666666666666666 			 1.0
    263 	 0.056179775280898875 				 0.696969696969697 			 0.0
    264 	 0.07865168539325842 				 0.36363636363636365 			 0.0
    265 	 0.0449438202247191 				 0.3181818181818181 			 0.0
    266 	 0.149438202247191 				 0.09090909090909091 			 1.0
    267 	 0.1348314606741573 				 0.36363636363636365 			 0.0
    268 	 0.0449438202247191 				 0.36363636363636365 			 0.0
    269 	 0.056179775280898875 				 0.3181818181818181 			 0.0
    270 	 0.12359550561797752 				 0.24242424242424246 			 0.0
    271 	 0.0449438202247191 				 0.3939393939393939 			 0.0
    272 	 0.07865168539325842 				 0.36363636363636365 			 0.0
    273 	 0.02247191011235955 				 0.3939393939393939 			 0.0
    274 	 0.056179775280898875 				 0.24242424242424246 			 0.0
    275 	 0.033707865168539325 				 0.36363636363636365 			 0.0
    276 	 0.06741573033707866 				 0.3181818181818181 			 0.0
    277 	 0.06741573033707866 				 0.36363636363636365 			 0.0
    278 	 0.02247191011235955 				 0.24242424242424246 			 0.0
    279 	 0.0898876404494382 				 0.36363636363636365 			 0.0
    280 	 0.056179775280898875 				 0.3939393939393939 			 0.0
    281 	 0.24719101123595508 				 0.3939393939393939 			 0.0
    282 	 0.3707865168539326 				 0.24242424242424246 			 0.0
    283 	 0.06741573033707866 				 0.36363636363636365 			 0.0
    284 	 0.033707865168539325 				 0.3939393939393939 			 0.0
    285 	 0.07865168539325842 				 0.3939393939393939 			 0.0
    286 	 0.1348314606741573 				 0.3181818181818181 			 0.0
    287 	 0.056179775280898875 				 0.6212121212121212 			 0.0
    288 	 0.06741573033707866 				 0.3181818181818181 			 0.0
    289 	 0.0449438202247191 				 0.36363636363636365 			 0.0
    290 	 0.033707865168539325 				 0.6212121212121212 			 0.0
    291 	 0.10112359550561797 				 0.3181818181818181 			 0.0
    292 	 0.056179775280898875 				 0.36363636363636365 			 0.0
    293 	 0.0449438202247191 				 0.3181818181818181 			 0.0
    294 	 0.06741573033707866 				 0.36363636363636365 			 0.0
    295 	 0.07865168539325842 				 0.36363636363636365 			 0.0
    296 	 0.033707865168539325 				 0.696969696969697 			 0.0
    297 	 0.10112359550561797 				 0.36363636363636365 			 0.0
    298 	 0.12359550561797752 				 0.4696969696969697 			 0.0
    

So, the features ranges are properly set. Now, before spliting, we have to check if the dataset is balanced.


```python
import numpy as np
result =  np.unique(y, return_counts=True)
ratio = result[1][1]/result[1][0]
print(f"The ratio of death event to survival in the dataset are {ratio}")
print("")
```

    The ratio of death event to survival in the dataset are 0.4729064039408867
    
    

So, we will use stratify spliting, to ensure all of train and test sets prepared with similar ratio.


```python
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X_scaled,y,stratify=y)
```

The following plot, shows the data that will be used to train the ML model.


```python
import matplotlib.pyplot as plt

X_train_points_x_axis = X_train["serum_creatinine"] 
X_train_points_y_axis = X_train["ejection_fraction"]

X_test_points_x_axis = X_test["serum_creatinine"] 
X_test_points_y_axis = X_test["ejection_fraction"]

color_selector = {
    0: 'blue', #survived
    1: 'red'   #dead
}
plt.figure(figsize=(10,10))
plt.scatter(
    X_train_points_x_axis,
    X_train_points_y_axis,
    s=50,
    c=[color_selector[y] for y in y_train],
    marker='o',
    label="Train data"
)
plt.scatter(
    X_test_points_x_axis,
    X_test_points_y_axis,
    s=50,
    marker='x',
    c=[color_selector[y] for y in y_test],
    label="Test data"
)
plt.xlabel('serum_creatinine')
plt.ylabel('ejection_fraction')
plt.axis('equal')
plt.grid(True)
plt.legend()
# --- Create custom legend for colors ---
legend_elements = [
    Line2D([0], [0], marker='o', color='w', label='Survived (0)', markerfacecolor='blue', markersize=8),
    Line2D([0], [0], marker='o', color='w', label='Dead (1)', markerfacecolor='red', markersize=8)
]

# Add both legends
first_legend = plt.legend(loc='upper right')  # Default legend (Train/Test)
plt.gca().add_artist(first_legend)  # Prevents overwriting
plt.legend(handles=legend_elements, loc='lower right')  # Color legend
plt.show()            
```


    
![png](output_33_0.png)
    


## Implementing KNN Model ##

We will create a KNN model and train it using the available data, prepared in the previous sections.


```python
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import confusion_matrix


def calc_metrics(tn, fp, fn, tp,parameter):
    if (parameter == 'accuracy'):
        return (tn+tp)/(tn+tp+fp+fn);
    elif (parameter == 'precision'):
        if (tp+fp != 0):
            return tp/(tp+fp);
        else:
            return 0;
    elif (parameter == 'recall'):
        return tp/(tp+fn);
    elif (parameter == 'f1'):
        precision = (tn+tp)/(tn+tp+fp+fn);
        recall = tp/(tp+fn);
        return (2*(precision*recall)/(precision+recall));
    return 0;


      
clf = KNeighborsClassifier(n_neighbors=4)
clf.fit(X_train,y_train)
tn,fp,fn,tp = confusion_matrix(y_test,clf.predict(X_test)).ravel()
accuracy = calc_metrics(tn,fp,fn,tp,'accuracy')
precision = calc_metrics(tn,fp,fn,tp,'precision')
recall = calc_metrics(tn,fp,fn,tp,'recall')
f1 = calc_metrics(tn,fp,fn,tp,'f1')

print(f"accuracy = {accuracy}")
print(f"precision = {precision}")
print(f"recall = {recall}")
print(f"f1 = {f1}")

```

    accuracy = 0.7333333333333333
    precision = 0.625
    recall = 0.4166666666666667
    f1 = 0.5314009661835749
    

The following table will show the test set and the correctness of prediction:


```python
import matplotlib.pyplot as plt

X_test_points_x_axis = X_test["serum_creatinine"] 
X_test_points_y_axis = X_test["ejection_fraction"]

y_predict = clf.predict(X_test)
y_predict_bool = y_test == y_predict
color_selector = {
    1: 'blue', #True
    0: 'red'   #False
}
plt.figure(figsize=(10,10))
plt.scatter(
    X_test_points_x_axis,
    X_test_points_y_axis,
    s=80,
    marker='s',
    c=[color_selector[y] for y in y_predict_bool],
    label='actual data',
)

plt.xlabel('serum_creatinine')
plt.ylabel('ejection_fraction')
plt.axis('equal')
plt.grid(True)
plt.legend()
# --- Create custom legend for colors ---
legend_elements = [
    Line2D([0], [0], marker='o', color='w', label='True', markerfacecolor='blue', markersize=8),
    Line2D([0], [0], marker='o', color='w', label='False', markerfacecolor='red', markersize=8)
]

# Add both legends
first_legend = plt.legend(loc='upper right')  # Default legend (Train/Test)
plt.gca().add_artist(first_legend)  # Prevents overwriting
plt.legend(handles=legend_elements, loc='lower right')  # Color legend
plt.show()            
```


    
![png](output_38_0.png)
    


## Hyperparameter Optimization ##

Now, to find out the best set of parameters, such as number of neighbors, method of estimating distance, weighting method, etc., we will handle few tests. The first parameter, is number of neighbors. The following code, compares the learning performance for different number of neighbors.

### k - number of neighbors ###


```python
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import confusion_matrix

n_neighbors_list = [1,4,8,16,32,64,128,200]
accuracy_list = []
precision_list = []
recall_list = []
f1_list = []
for n in n_neighbors_list:
    clf = KNeighborsClassifier(n_neighbors=n)
    clf.fit(X_train,y_train)
    tn,fp,fn,tp = confusion_matrix(y_test,clf.predict(X_test)).ravel()
    accuracy_list.append(calc_metrics(tn,fp,fn,tp,'accuracy'))
    precision_list.append(calc_metrics(tn,fp,fn,tp,'precision'))
    recall_list.append(calc_metrics(tn,fp,fn,tp,'recall'))
    f1_list.append(calc_metrics(tn,fp,fn,tp,'f1'))


plt.figure(figsize=(10,10))
plt.plot(n_neighbors_list,accuracy_list,c='red',label='accuracy')
plt.plot(n_neighbors_list,precision_list,c='green',label='precision')
plt.plot(n_neighbors_list,recall_list,c='blue',label='recall')
plt.plot(n_neighbors_list,f1_list,c='black',label='f1')
plt.grid(True,alpha=0.8)
plt.legend()
plt.show()
```


    
![png](output_42_0.png)
    


According to this chart, the performance of our model seems to have an extreme in n between <span style="font-weight:bold"> 8 and 16 </span>. for larger number of neighbors, all of the metircs fall down. <br>

### Estimating the Distance method ###


The next parameter to check is the method of estimating distance, that could be any of the followings:

<ul>
    <li>cityblock</li>
    <li>cosine</li>
    <li>euclidean</li>
    <li>haversine</li>
    <li>l1</li>
    <li>l2</li>
    <li>manhattan</li>
    <li>nan_euclidean</li>
    <li>minkowski</li>
</ul>

We will check for all of them, and find out which one is the best for our problem.



```python
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import confusion_matrix

# Your existing data collection code
metrics_list = ['cityblock','cosine','euclidean','haversine','l1','l2','manhattan','nan_euclidean','minkowski']
accuracy_list = []
precision_list = []
recall_list = []
f1_list = []

for metric in metrics_list:
    if (metric == 'minkowski'):
        clf = KNeighborsClassifier(n_neighbors=16, metric=metric,p=3)
    else:
        clf = KNeighborsClassifier(n_neighbors=16, metric=metric)
    clf.fit(X_train, y_train)
    tn, fp, fn, tp = confusion_matrix(y_test, clf.predict(X_test)).ravel()
    accuracy_list.append(calc_metrics(tn, fp, fn, tp, 'accuracy'))
    precision_list.append(calc_metrics(tn, fp, fn, tp, 'precision'))
    recall_list.append(calc_metrics(tn, fp, fn, tp, 'recall'))
    f1_list.append(calc_metrics(tn, fp, fn, tp, 'f1'))

# Create a DataFrame for the heatmap
data = {
    'Metric': metrics_list,
    'Accuracy': accuracy_list,
    'Precision': precision_list,
    'Recall': recall_list,
    'F1': f1_list
}
df = pd.DataFrame(data).set_index('Metric')

# Plot the heatmap
plt.figure(figsize=(8, 6))
sns.heatmap(
    df.T,  # Transpose to show metrics as columns
    annot=True,  # Show values in cells
    fmt=".3f",  # Format to 3 decimal places
    cmap="YlGnBu",  # Color map (Yellow-Green-Blue)
    linewidths=0.5,
    cbar_kws={'label': 'Score'}
)
plt.title('Performance Metrics by Distance Metric')
plt.xlabel('Distance Metric')
plt.ylabel('Performance Metric')
plt.xticks(rotation=45, ha='right')  # Rotate x-axis labels
plt.tight_layout()
plt.show()
```


    
![png](output_46_0.png)
    


According to the results, the lowest values of performance indicators belongs to:
<ul>
    <li>Accuracy: Haversine and Cosine</li>
    <li>Precision: Haversine and Cosine </li>
    <li>Recall: Haversine </li>
    <li>F1-score: Haversine and Cosine </li>
</ul>
and the best values:
<ul>
    <li>Accuracy: minkowski(p=3)</li>
    <li>Precision: minkowski(p=3) </li>
    <li>Recall: minkowski(p=3), manhattan, l1, and cityblock</li>
    <li>F1-score: minkowski(p=3) </li>
</ul>

So, Minkowski method with p=3, seems to be the best, and cosine and Harvsine the worst. Both of these two mentioned method, are idealy usefull for different applications. Cosine method is useful, when the angle between points is important. but here it is not the case. The Harvsine is usefull for geospatial data (for example when the data points are specified with latitude and longitude coordinates), and this is not also the case, now.

### Weighting Method ###

Now, we will compare the two possible methods of weighting, that are <span style="font-weight:bold">uniform </span>and <span style="font-weight:bold">distance</span>.


```python
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import confusion_matrix

weighting_methods_list = ['uniform','distance']
accuracy_list = []
precision_list = []
recall_list = []
f1_list = []

for weight in weighting_methods_list:
    clf = KNeighborsClassifier(n_neighbors=16, metric='minkowski',p=3,weights=weight)
    clf.fit(X_train, y_train)
    tn, fp, fn, tp = confusion_matrix(y_test, clf.predict(X_test)).ravel()
    accuracy_list.append(calc_metrics(tn, fp, fn, tp, 'accuracy'))
    precision_list.append(calc_metrics(tn, fp, fn, tp, 'precision'))
    recall_list.append(calc_metrics(tn, fp, fn, tp, 'recall'))
    f1_list.append(calc_metrics(tn, fp, fn, tp, 'f1'))

data = {
    'Weight': weighting_methods_list,
    'Accuracy': accuracy_list,
    'Precision': precision_list,
    'Recall': recall_list,
    'F1': f1_list
}
df = pd.DataFrame(data).set_index('Weight')

plt.figure(figsize=(8, 6))
sns.heatmap(
    df.T,  # Transpose to show metrics as columns
    annot=True,  # Show values in cells
    fmt=".3f",  # Format to 3 decimal places
    cmap="YlGnBu",  # Color map (Yellow-Green-Blue)
    linewidths=0.5,
    cbar_kws={'label': 'Score'}
)
plt.title('Performance Metrics by Weighting Method')
plt.xlabel('Weighting Method')
plt.ylabel('Performance Metric')
plt.xticks(rotation=45, ha='right')  # Rotate x-axis labels
plt.tight_layout()
plt.show()
```


    
![png](output_50_0.png)
    


According to the results, the Accuracy and Precision for uniform weighting is better, but recall and F1 of distance are higher. <br>
the following chart shows the predictions for uniform and distance weighting.


```python

clf = KNeighborsClassifier(n_neighbors=16, metric='minkowski',p=3,weights='uniform')
clf.fit(X_train, y_train)
y_predict_uniform = clf.predict(X_test)
y_predict_uniform_bool = y_test == y_predict_uniform

clf = KNeighborsClassifier(n_neighbors=16, metric='minkowski',p=3,weights='distance')
clf.fit(X_train, y_train)
y_predict_distance = clf.predict(X_test)
y_predict_distance_bool = y_test == y_predict_distance


X_test_points_x_axis = X_test["serum_creatinine"] 
X_test_points_y_axis = X_test["ejection_fraction"]



color_selector = {
    1: 'blue', #True
    0: 'red'   #False
}
fig, (ax1,ax2) = plt.subplots(1,2, figsize=(12,5))

ax1.scatter(
    X_test_points_x_axis,
    X_test_points_y_axis,
    s=80,
    marker='s',
    c=[color_selector[y] for y in y_predict_uniform_bool],
    label='uniform',
)

ax1.set_xlabel('serum_creatinine')
ax1.set_ylabel('ejection_fraction')
ax1.axis('equal')
ax1.grid(True)
ax1.set_title('uniform')

ax2.scatter(
    X_test_points_x_axis,
    X_test_points_y_axis,
    s=80,
    marker='s',
    c=[color_selector[y] for y in y_predict_distance_bool],
    label='distance',
)

ax2.set_xlabel('serum_creatinine')
ax2.set_ylabel('ejection_fraction')
ax2.axis('equal')
ax2.grid(True)
ax2.set_title('distance')


plt.show()
```


    
![png](output_52_0.png)
    


So, the following values for model parameters are selected:
<ul>
    <li>number of neighbors: 16</li>
    <li>Method of estimating distance: Minkowski with p=3</li>
    <li>Weighting method: Distance </li>
</ul>

## Model Evaluation ##

The model with the prepared data and selected parameters is:


```python
from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import confusion_matrix

df = pd.read_csv("heart_failure_clinical_records_dataset.csv")
X = df[["serum_creatinine","ejection_fraction"]]
y = df["DEATH_EVENT"]

scaler = MinMaxScaler()
X_scaled = scaler.fit_transform(X)
X_scaled = pd.DataFrame(X_scaled, columns=['serum_creatinine', 'ejection_fraction'])
X_train, X_test, y_train, y_test = train_test_split(X_scaled,y,stratify=y)

clf = KNeighborsClassifier(n_neighbors=16, metric='minkowski',p=3,weights='distance')
clf.fit(X_train, y_train)
y_predict = clf.predict(X_test)
```

The predictions on the test data are:


```python
import matplotlib.pyplot as plt

X_test_points_x_axis = X_test["serum_creatinine"] 
X_test_points_y_axis = X_test["ejection_fraction"]

y_predict = clf.predict(X_test)
y_predict_bool = y_test == y_predict
color_selector = {
    1: 'blue', #True
    0: 'red'   #False
}
plt.figure(figsize=(10,10))
plt.scatter(
    X_test_points_x_axis,
    X_test_points_y_axis,
    s=80,
    marker='s',
    c=[color_selector[y] for y in y_predict_bool],
    label='actual data',
)

plt.xlabel('serum_creatinine')
plt.ylabel('ejection_fraction')
plt.axis('equal')
plt.grid(True)
plt.legend()
# --- Create custom legend for colors ---
legend_elements = [
    Line2D([0], [0], marker='o', color='w', label='True', markerfacecolor='blue', markersize=8),
    Line2D([0], [0], marker='o', color='w', label='False', markerfacecolor='red', markersize=8)
]

# Add both legends
first_legend = plt.legend(loc='upper right')  # Default legend (Train/Test)
plt.gca().add_artist(first_legend)  # Prevents overwriting
plt.legend(handles=legend_elements, loc='lower right')  # Color legend
plt.show()            
```


    
![png](output_58_0.png)
    


We will use the following metrics, to evaluate the model:
<ul>
    <li>Confusion Matrix</li>
    <li>Accuracy</li>
    <li>Precision</li>
    <li>Recall</li>
    <li>F1 score</li>
</ul>


```python
from sklearn.metrics import confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt


y_pred = clf.predict(X_test)
cm = confusion_matrix(y_test, y_pred)
plt.figure(figsize=(6, 4))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=['Predicted Alive', 'Predicted Dead'],
            yticklabels=['Actual Alive', 'Actual Dead'])
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.title('KNN Model Confusion Matrix')
plt.show()

```


    
![png](output_60_0.png)
    



```python


def calc_metrics(tn, fp, fn, tp,parameter):
    if (parameter == 'accuracy'):
        return (tn+tp)/(tn+tp+fp+fn);
    elif (parameter == 'precision'):
        if (tp+fp != 0):
            return tp/(tp+fp);
        else:
            return 0;
    elif (parameter == 'recall'):
        return tp/(tp+fn);
    elif (parameter == 'f1'):
        precision = (tn+tp)/(tn+tp+fp+fn);
        recall = tp/(tp+fn);
        return (2*(precision*recall)/(precision+recall));
    return 0;

tn, fp, fn, tp = confusion_matrix(y_test, clf.predict(X_test)).ravel()

accuracy = calc_metrics(tn, fp, fn, tp, 'accuracy')
precision = calc_metrics(tn, fp, fn, tp, 'precision')
recall = calc_metrics(tn, fp, fn, tp, 'recall')
f1 = calc_metrics(tn, fp, fn, tp, 'f1')
print(f"accuracy = {accuracy:.2f}")
print(f"precision = {precision:.2f}")
print(f"recall = {recall:.2f}")
print(f"f1 = {f1:.2f}")
```

    accuracy = 0.72
    precision = 0.57
    recall = 0.50
    f1 = 0.59
    

## Comparison with the Original Paper ##

the following data shows the performance metrics of the current project with the original paper [2]


```python
accuracy_ref2 = 0.624
f1_ref2 = 0.148
data = {
    "": ["Current","Ref [2]"],
    "Accuracy": [accuracy,accuracy_ref2],
    "F1 Score": [f1,f1_ref2]
}
df_comp = pd.DataFrame(data)
df_comp
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th></th>
      <th>Accuracy</th>
      <th>F1 Score</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>Current</td>
      <td>0.720</td>
      <td>0.590164</td>
    </tr>
    <tr>
      <th>1</th>
      <td>Ref [2]</td>
      <td>0.624</td>
      <td>0.148000</td>
    </tr>
  </tbody>
</table>
</div>



According to the above table, our scores are much better than thos provided by [2] for KNN model.

## Conclusion & Future Work ##

The project revealed that we have the ability to handle such projects, using available tools. Our results show much more better than the main reference. It could be because of the parameter optimization process, that was handled in this project. <br>
We can handle the following improvments for future works or revisions:
<ul>
    <li>Implement the <span style="font-weight:bold">Feature Selection</span> section, instead of using the ref. data</li>
    <li>Improve the analysis and resaoning for results</li>
    <li>Add literature review</li>
    <li>Improve the codes and coding style, and make the code clean</li>
    <li>Implement more ML methods for this problem</li>
    <li>Add more performane metrics</li>
</ul>

## References ##

<ol>
    <li>
        "Heart Failure Clinical Records," UCI Machine Learning Repository, 2020. [Online]. Available: https://doi.org/10.24432/C5Z89R.        
    </li>
    <li>
        Chicco, D., Jurman, G. Machine learning can predict survival of patients with heart failure from serum creatinine and ejection fraction alone. BMC Med Inform Decis Mak 20, 16 (2020). https://doi.org/10.1186/s12911-020-1023-5
    </li>
</ol>



```python

```
